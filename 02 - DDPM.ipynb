{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J45M5o5XrUrk"
      },
      "source": [
        "# Goal:\n",
        "\n",
        "In this assignment, you will implement a [DDPM](https://arxiv.org/abs/2006.11239) class on MNIST dataset using PyTorch according to the guidence. The goal is to minimize the loss function and train the model to generate MNIST images.\n",
        "\n",
        "The `Train` and `UNet` classes are already implemented for you. You need to implement the `DDPM` class (see details below) according to what is covered in the lecture ([slides](https://www.dropbox.com/s/0gu91rovro71q90/Diffusion.pdf?dl=0)). The images generated by the model will be automatically shown according to the `Trainer` class implementation. Make sure the generated images are shown in the output, it will be graded.\n",
        "\n",
        "Grade:\n",
        "- **DDPM class implementation (20 points).**\n",
        "- **Trainer class completion (10 points)**\n",
        "- **Training the model to generate reasonable Digits images within 20 epochs (10 points).**\n",
        "- **Write a report to describe:**\n",
        "     - **the sample images generated by each epochs (5 points)**\n",
        "     - **the images generated at different steps t by the trained model (5 points)**\n",
        "\n",
        "**Please note that the function to generate the images is already provided.**\n",
        "\n",
        "---\n",
        "Please DO NOT change the code provided, only add your own code where indicated. It is recommended that you **use CPU session to debug** when GPU is not necessary since Colab only gives 12 hrs of free GPU access at a time. If you use up the GPU resource, you may consider using Kaggle GPU resource. Thank you and good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtibzCT8XdVU"
      },
      "source": [
        "# Pre-determined config and given functions (no need to change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1liMpqHzQbt1",
        "outputId": "99c820d9-c625-4443-af6d-e20fd07cfcfb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4reTlFisQdtM"
      },
      "source": [
        "Add the following files to your directory:\n",
        "- args.py\n",
        "- unet.py\n",
        "- dataset.py\n",
        "- utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting easydict==1.13\n",
            "  Using cached easydict-1.13-py3-none-any.whl.metadata (4.2 kB)\n",
            "Using cached easydict-1.13-py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: easydict\n",
            "Successfully installed easydict-1.13\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install easydict==1.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VjXxywmkQv0P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from typing import Tuple, Optional\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "from ddpm_utils.args import *\n",
        "from ddpm_utils.dataset import *\n",
        "from ddpm_utils.unet import *\n",
        "from q2_trainer_ddpm import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OOqcQgdQZcW",
        "outputId": "f43f104a-e0e2-4ce3-a532-5786b8254a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu backend\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using {args.device} backend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeTX8umzQZcX"
      },
      "source": [
        "# Finish the DenoiseDiffusion model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kghSEXNQZcX"
      },
      "source": [
        "<img src=\"images/diffusion_model.png\" width=800px height=500px />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW9Mu-QFXjcO"
      },
      "source": [
        "We initialize ${\\epsilon_\\theta}(x_t, t)$, $\\beta_1, \\dots, \\beta_T$ (linearly increasing variance schedule), $\\alpha_t = 1 - \\beta_t$, $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$, $\\sigma^2 = \\beta$\n",
        "```python\n",
        "class DenoiseDiffusion:\n",
        "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.eps_model = eps_model\n",
        "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.n_steps = n_steps\n",
        "        self.sigma2 = self.beta\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7pBRL-GZza-"
      },
      "source": [
        "## q_sample\n",
        "\n",
        "We need to implement the function to get samples from $q(x_t|x_0)$.\n",
        "\n",
        "\\begin{align}\n",
        "q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
        "\\end{align}\n",
        "\n",
        "Hint: sampling from $\\mathcal{N} \\Big(\\mu, \\sigma^2\\Big)$ is the same as sampling from $\\mathcal{N} \\Big(0, I\\Big)$ then scale and shift.\n",
        "\n",
        "To do so, we need to implment the function:\n",
        "```python\n",
        "    def q_xt_x0(\n",
        "        self, x0: torch.Tensor, t: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        ...\n",
        "        return mean, var\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```python\n",
        "    def q_sample(\n",
        "        self, x0: torch.Tensor, t: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        ...\n",
        "        return sample\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdb8-T3Ea0i8"
      },
      "source": [
        "## p_sample\n",
        "We need to implement the function to get samples from ${p_\\theta}(x_{t-1}|x_t)$\n",
        "\n",
        "\\begin{align}\n",
        "{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n",
        "{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n",
        "{\\mu_\\theta}(x_t, t)\n",
        "  &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
        "    \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}{\\epsilon_\\theta}(x_t, t) \\Big)\n",
        "\\end{align}\n",
        "\n",
        "*   `beta` is defined as $1-\\alpha_t$  \n",
        "*   `eps_coef` is defined as $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n",
        "*   `mu_theta` is defined as $\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t) \\Big)$\n",
        "*   `var` is defined as $\\sigma_t^2 \\mathbf{I} = \\beta_t \\mathbf{I}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4w9VW-LQZcX"
      },
      "source": [
        "To do so, we need to implement the functions:\n",
        "```python\n",
        "    def p_xt_prev_xt(\n",
        "        self, xt: torch.Tensor, t: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        ...\n",
        "        return mu_theta, var\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```python\n",
        "    def p_sample(\n",
        "        self, xt: torch.Tensor, t: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        ...\n",
        "        return sample\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DlpUzpcgJdt"
      },
      "source": [
        "## loss\n",
        "We need to implment the function to get the loss:\n",
        "$$L(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n",
        "\\epsilon - {\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
        "\\bigg\\Vert^2 \\Bigg]$$\n",
        "\n",
        "where `x_t` is sampled from $q(x_t|x_0)$ which is given by $\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b1mAVxIsTw1y"
      },
      "outputs": [],
      "source": [
        "class DenoiseDiffusion():\n",
        "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.eps_model = eps_model\n",
        "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.n_steps = n_steps\n",
        "        self.sigma2 = self.beta\n",
        "\n",
        "\n",
        "    ### UTILS\n",
        "    def gather(self, c: torch.Tensor, t: torch.Tensor):\n",
        "        c_ = c.gather(-1, t)\n",
        "        return c_.reshape(-1, 1, 1, 1)\n",
        "\n",
        "    ### FORWARD SAMPLING\n",
        "    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Return mean and variance of q(x_t|x_0)\n",
        "        # mean = sqrt(alpha_bar) * x0\n",
        "        # var = (1 - alpha_bar)\n",
        "        \n",
        "        alpha_bar = self.gather(self.alpha_bar, t)\n",
        "        mean = torch.sqrt(alpha_bar) * x0\n",
        "        var = 1.0 - alpha_bar\n",
        "        \n",
        "        return mean, var\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n",
        "        if eps is None:\n",
        "            eps = torch.randn_like(x0)\n",
        "        \n",
        "        # Return x_t sampled from q(x_t|x_0)\n",
        "        # x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * epsilon\n",
        "        \n",
        "        alpha_bar = self.gather(self.alpha_bar, t)\n",
        "        mean = torch.sqrt(alpha_bar) * x0\n",
        "        std = torch.sqrt(1.0 - alpha_bar)\n",
        "        \n",
        "        sample = mean + std * eps\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    ### REVERSE SAMPLING\n",
        "    def p_xt_prev_xt(self, xt: torch.Tensor, t: torch.Tensor):\n",
        "        # Return mean and variance of p_theta(x_{t-1} | x_t)\n",
        "        # mu_theta = (1/sqrt(alpha_t)) * (x_t - (beta_t/sqrt(1-alpha_bar_t)) * epsilon_theta)\n",
        "        # var = beta_t\n",
        "        \n",
        "        # Predict noise using the model (make sure t is properly formatted)\n",
        "        eps_theta = self.eps_model(xt, t)\n",
        "        \n",
        "        # Gather the right values from alpha, beta, and alpha_bar using the time step t\n",
        "        alpha = self.gather(self.alpha, t)\n",
        "        beta = self.gather(self.beta, t)\n",
        "        alpha_bar = self.gather(self.alpha_bar, t)\n",
        "        \n",
        "        # Calculate the coefficient for eps_theta\n",
        "        eps_coef = beta / torch.sqrt(1.0 - alpha_bar)\n",
        "        \n",
        "        # Calculate mu_theta according to the formula\n",
        "        mu_theta = (1.0 / torch.sqrt(alpha)) * (xt - eps_coef * eps_theta)\n",
        "        \n",
        "        # The variance is simply beta\n",
        "        var = beta\n",
        "        \n",
        "        return mu_theta, var\n",
        "\n",
        "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor, set_seed=False):\n",
        "        if set_seed:\n",
        "            torch.manual_seed(42)\n",
        "        \n",
        "        # Sample x_{t-1} from p_theta(x_{t-1}|x_t)\n",
        "        # Get mean and variance\n",
        "        mu_theta, var = self.p_xt_prev_xt(xt, t)\n",
        "        \n",
        "        # No noise if t = 0 (final step)\n",
        "        if t[0] == 0:\n",
        "            return mu_theta\n",
        "        \n",
        "        # Add noise with the variance\n",
        "        sigma = torch.sqrt(var)\n",
        "        \n",
        "        # Generate noise\n",
        "        noise = torch.randn_like(xt)\n",
        "        \n",
        "        # Sample from the distribution\n",
        "        sample = mu_theta + sigma * noise\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None, set_seed=False):\n",
        "        \"\"\"Compute the DDPM loss according to the formula.\"\"\"\n",
        "        if set_seed:\n",
        "            torch.manual_seed(42)\n",
        "        batch_size = x0.shape[0]\n",
        "        t = torch.randint(\n",
        "            0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long\n",
        "        )\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        \n",
        "        # Calculate noisy sample x_t \n",
        "        alpha_bar = self.gather(self.alpha_bar, t)\n",
        "        sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
        "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)\n",
        "        \n",
        "        # Generate x_t from x_0 and noise according to the formula\n",
        "        x_t = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
        "        \n",
        "        # Predict noise\n",
        "        predicted_noise = self.eps_model(x_t, t)\n",
        "        \n",
        "        # Alternative loss calculation - use L2 norm squared directly\n",
        "        squared_error = torch.sum((noise - predicted_noise) ** 2, dim=list(range(1, noise.ndim)))\n",
        "        loss = torch.mean(squared_error)\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cJ2_T5IQZcX"
      },
      "source": [
        "# Build trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S-xi6zsCQZcX"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args, eps_model, diffusion_model):\n",
        "\n",
        "        self.eps_model = eps_model.to(args.device)\n",
        "\n",
        "        self.diffusion = diffusion_model\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.eps_model.parameters(), lr=args.learning_rate\n",
        "        )\n",
        "        self.args = args\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.ema = EMA(0.995)\n",
        "        self.ema_model = copy.deepcopy(self.eps_model).eval().requires_grad_(False)\n",
        "\n",
        "\n",
        "\n",
        "    def train_epoch(self, dataloader, scaler):\n",
        "        current_lr = round(self.optimizer.param_groups[0]['lr'], 5)\n",
        "        i = 0\n",
        "        running_loss = 0.\n",
        "        with tqdm(range(len(dataloader)), desc=f'Epoch : - lr: - Loss :') as progress:\n",
        "            for x0 in dataloader:\n",
        "                i += 1\n",
        "                # Move data to device\n",
        "                x0 = x0.to(self.args.device)\n",
        "                # Calculate the loss\n",
        "                with autocast(device_type=args.device, enabled=self.args.fp16_precision):\n",
        "                    loss = self.diffusion.loss(x0)\n",
        "                \n",
        "                # Zero gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                # Backward pass\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "                self.ema.step_ema(self.ema_model, self.eps_model)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                self.loss_per_iter.append(running_loss / i)\n",
        "                progress.update()\n",
        "                progress.set_description(f'Epoch: {self.current_epoch}/{self.args.epochs} - lr: {current_lr} - Loss: {round(running_loss / i, 2)}')\n",
        "            progress.set_description(f'Epoch: {self.current_epoch}/{self.args.epochs} - lr: {current_lr} - Loss: {round(running_loss / len(dataloader), 2)}')\n",
        "\n",
        "            # Step the scheduler after each epoch\n",
        "            self.scheduler.step()\n",
        "\n",
        "\n",
        "    def train(self, dataloader):\n",
        "            scaler = GradScaler(device=self.args.device, enabled=self.args.fp16_precision)\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
        "            start_epoch = self.current_epoch\n",
        "            self.loss_per_iter = []\n",
        "            for current_epoch in range(start_epoch, self.args.epochs):\n",
        "                self.current_epoch = current_epoch\n",
        "                self.train_epoch(dataloader, scaler)\n",
        "\n",
        "                if current_epoch % self.args.show_every_n_epochs == 0:\n",
        "                    self.sample()\n",
        "\n",
        "                if (current_epoch + 1) % self.args.save_every_n_epochs == 0:\n",
        "                    self.save_model()\n",
        "\n",
        "\n",
        "    def sample(self, n_steps=None, set_seed=False):\n",
        "        if set_seed:\n",
        "            torch.manual_seed(42)\n",
        "        if n_steps is None:\n",
        "            n_steps = self.args.n_steps\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n",
        "            x = torch.randn(\n",
        "                [\n",
        "                    self.args.n_samples,\n",
        "                    self.args.image_channels,\n",
        "                    self.args.image_size,\n",
        "                    self.args.image_size,\n",
        "                ],\n",
        "                device=self.args.device,\n",
        "            )\n",
        "            if self.args.nb_save is not None:\n",
        "                saving_steps = [self.args.n_steps - 1]\n",
        "            # Remove noise for $T$ steps\n",
        "            for t_ in tqdm(range(n_steps)):\n",
        "                # We go from t=T-1 to t=0\n",
        "                t = torch.full((self.args.n_samples,), n_steps - 1 - t_, device=self.args.device, dtype=torch.long)\n",
        "                # Get x_{t-1} from p_theta(x_{t-1}|x_t)\n",
        "                x = self.diffusion.p_sample(x, t)\n",
        "            \n",
        "                if self.args.nb_save is not None and t_ in saving_steps:\n",
        "                    print(f\"Showing/saving samples from epoch {self.current_epoch}\")\n",
        "                    self.show_save(\n",
        "                        x,\n",
        "                        show=True,\n",
        "                        save=True,\n",
        "                        file_name=f\"DDPM_epoch_{self.current_epoch}_sample_{t_}.png\",\n",
        "                    )\n",
        "        return x\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save({\n",
        "                'epoch': self.current_epoch,\n",
        "                'model_state_dict': self.eps_model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                }, args.MODEL_PATH)\n",
        "\n",
        "    def show_save(self, img_tensor, show=True, save=True, file_name=\"sample.png\"):\n",
        "        fig, axs = plt.subplots(3, 3, figsize=(10, 10))  # Create a 4x4 grid of subplots\n",
        "        assert img_tensor.shape[0] >= 9, \"Number of images should be at least 9\"\n",
        "        img_tensor = img_tensor[:9]\n",
        "        for i, ax in enumerate(axs.flat):\n",
        "            # Remove the channel dimension and convert to numpy\n",
        "            img = img_tensor[i].squeeze().cpu().numpy()\n",
        "\n",
        "            ax.imshow(img, cmap=\"gray\")  # Display the image in grayscale\n",
        "            ax.axis(\"off\")  # Hide the axis\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save:\n",
        "            plt.savefig('images/' + file_name)\n",
        "        if show:\n",
        "            plt.show()\n",
        "        plt.close(fig)\n",
        "        \n",
        "        \n",
        "    def generate_intermediate_samples(self, model=None, n_samples=4, img_size=32, steps_to_show=[0,999], n_steps=None, set_seed=False):\n",
        "        \"\"\"\n",
        "        Generate multiple images and return intermediate steps of the diffusion process\n",
        "        Args:\n",
        "            model: The trained diffusion model (unused parameter for compatibility)\n",
        "            n_samples: Number of images to generate\n",
        "            img_size: Size of the images (assumes square images)\n",
        "            steps_to_show: Steps at which to capture the intermediate results\n",
        "            n_steps: Total number of diffusion steps\n",
        "        Returns:\n",
        "            List of tensors representing the images at different steps\n",
        "        \"\"\"\n",
        "        \n",
        "        if set_seed:\n",
        "            torch.manual_seed(42)\n",
        "        \n",
        "        if n_steps is None:\n",
        "            n_steps = self.args.n_steps\n",
        "            \n",
        "        # Start from random noise\n",
        "        x = torch.randn(n_samples, 1, img_size, img_size, device=self.args.device, requires_grad=False)\n",
        "\n",
        "        # Store images at each step we want to show\n",
        "        images = []\n",
        "        images.append(x.detach().cpu().numpy())  # Initial noise\n",
        "\n",
        "        with torch.no_grad():  # Make sure we don't accumulate gradients\n",
        "            for step in tqdm(range(1, n_steps+1, 1)):\n",
        "                # Calculate the time step in the reverse process (going from T to 0)\n",
        "                t = torch.full((n_samples,), n_steps - step, device=self.args.device, dtype=torch.long)\n",
        "                \n",
        "                # Sample from the model\n",
        "                x = self.diffusion.p_sample(x, t)\n",
        "            \n",
        "                # Store intermediate result if it's a step we want to display\n",
        "                if step in steps_to_show:\n",
        "                    images.append(x.detach().cpu().numpy())\n",
        "\n",
        "        return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu9GxyrWQZcY"
      },
      "source": [
        "# Build backbone model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugjkZpaQQZcY"
      },
      "source": [
        "The most common choice of NN architecture for image diffusion models is the U-Net, which gets its name from the U-shape of the architecture as shown in the diagram below.\n",
        "\n",
        "Like an autoencoder, a U-Net consists of several downsampling stages in which the filter dimension of the image representation is first increased and then the spatial dimensions are downsampled, a bottleneck, and then several upsampling stages in which these downsampling transformations are reversed. The main difference between a U-Net and a standard autoencoder architecture is that at each stage in the upsampling path with include the original representation from the corresponding downsampling stage.\n",
        "\n",
        "One motivation for using a U-Net rather than a standard autoencoder is that the final representation has information from various frequencies. Another is that the ResNet-like connections in the U-Net help with training due to improved gradient propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDqpraOxQZcY"
      },
      "source": [
        "<img src=\"images/téléchargement.jpeg\" width=800px height=400px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbICqs6TdcnQ",
        "outputId": "d9449d26-05b3-499c-9b4d-d8efeb0be422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No weights to load\n"
          ]
        }
      ],
      "source": [
        "eps_model = UNet(c_in=1,c_out=1)\n",
        "eps_model = load_weights(eps_model, args.MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z59rxXJal-hX"
      },
      "source": [
        "# Start training when you finish filling the code above\n",
        "Expected time: About `40s` for each epoch (`15 epoches` in total), if you don't change the config parameters. No model-checkpoint-saving logic is implemented. Please feel free to implement it if you need it. There will be samples displayed and saved (in `.png` images) during training for every epoch. You should be able to find the saved images in the `Files` on the left hand side if you are using Google colab.\n",
        "\n",
        "Notice: `15 epoches` in total is just a safe setting to generate MNIST-style images. Usually, it should start to generate interpretable images around `6 epoches` with loss around `16`. If you don't see this, there may be something wrong with your implementation. Please double check your code before trying to having more epoches of training. Thanks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Cdm6ZiTw1y",
        "outputId": "363266ce-f1c8-4818-f329-42b382195cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .\\MNISTDataset\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting .\\MNISTDataset\\raw\\train-images-idx3-ubyte.gz to .\\MNISTDataset\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .\\MNISTDataset\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.53MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting .\\MNISTDataset\\raw\\train-labels-idx1-ubyte.gz to .\\MNISTDataset\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .\\MNISTDataset\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.55MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting .\\MNISTDataset\\raw\\t10k-images-idx3-ubyte.gz to .\\MNISTDataset\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .\\MNISTDataset\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.02MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting .\\MNISTDataset\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNISTDataset\\raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "diffusion_model = DenoiseDiffusion(\n",
        "            eps_model=eps_model,\n",
        "            n_steps=args.n_steps,\n",
        "            device=args.device,\n",
        "        )\n",
        "\n",
        "trainer = Trainer(args, eps_model, diffusion_model)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    MNISTDataset(),\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I5Aq0VZSQZcY",
        "outputId": "43d4bb50-8324-4e50-8681-fcb9c10e6f81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 0/20 - lr: 0.0002 - Loss: 811.34:   3%|▎         | 7/234 [05:22<2:54:22, 46.09s/it] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m current_epoch\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshow_every_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample()\n",
            "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, dataloader, scaler)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfp16_precision):\n\u001b[1;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "Cell \u001b[1;32mIn[6], line 112\u001b[0m, in \u001b[0;36mDenoiseDiffusion.loss\u001b[1;34m(self, x0, noise, set_seed)\u001b[0m\n\u001b[0;32m    109\u001b[0m x_t \u001b[38;5;241m=\u001b[39m sqrt_alpha_bar \u001b[38;5;241m*\u001b[39m x0 \u001b[38;5;241m+\u001b[39m sqrt_one_minus_alpha_bar \u001b[38;5;241m*\u001b[39m noise\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Predict noise\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Alternative loss calculation - use L2 norm squared directly\u001b[39;00m\n\u001b[0;32m    115\u001b[0m squared_error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((noise \u001b[38;5;241m-\u001b[39m predicted_noise) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, noise\u001b[38;5;241m.\u001b[39mndim)))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\akobe\\Downloads\\Teaching_IFT6135-Assignment3-H25\\ddpm_utils\\unet.py:166\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    164\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_dim)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet_forwad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\akobe\\Downloads\\Teaching_IFT6135-Assignment3-H25\\ddpm_utils\\unet.py:142\u001b[0m, in \u001b[0;36mUNet.unet_forwad\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munet_forwad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m    141\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minc(x)\n\u001b[1;32m--> 142\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m#x2 = self.sa1(x2)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x2, t)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\akobe\\Downloads\\Teaching_IFT6135-Assignment3-H25\\ddpm_utils\\unet.py:69\u001b[0m, in \u001b[0;36mDown.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layer(t)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m emb\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\akobe\\Downloads\\Teaching_IFT6135-Assignment3-H25\\ddpm_utils\\unet.py:48\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mgelu(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_conv(x))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJAOdPmQZcY"
      },
      "source": [
        "# Generate intermediate images from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq7Vc44Fbbhr",
        "outputId": "2159c59a-65c7-4483-ac9b-6794cbb70012"
      },
      "outputs": [],
      "source": [
        "steps_to_show = list(range(0, args.n_steps, 100)) + [args.n_steps-1]\n",
        "steps_to_show = [0, 100, 500, 800, 900, 950, 980, 999]\n",
        "images = trainer.generate_intermediate_samples(n_samples=4, steps_to_show=steps_to_show)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Ao44aWhUTw1z",
        "outputId": "8e0f0213-6109-4e1d-9149-5fc4dc867a7d"
      },
      "outputs": [],
      "source": [
        "def plot_intermediate_samples(images, steps_to_show, n_samples):\n",
        "    \"\"\"\n",
        "    Plot the intermediate steps of the diffusion process\n",
        "    Args:\n",
        "        images: List of image tensors at different steps\n",
        "        steps_to_show: List of steps that were captured\n",
        "        n_samples: Number of images to show\n",
        "    \"\"\"\n",
        "    # Create a figure with n_samples rows and len(steps_to_show) columns\n",
        "    plt.figure(figsize=(25, 15*n_samples))\n",
        "    fig, axs = plt.subplots(n_samples, len(steps_to_show))\n",
        "    # Plot each image\n",
        "    for sample_idx in range(n_samples):\n",
        "        for step_idx, img in enumerate(images):\n",
        "            axs[sample_idx, step_idx].imshow(img[sample_idx, 0], cmap='gray')\n",
        "            step = steps_to_show[step_idx] if step_idx < len(steps_to_show) else args.n_steps\n",
        "            axs[sample_idx, step_idx].set_title(f' Image {sample_idx} \\nt={args.n_steps - step-1}',size=8)\n",
        "            axs[sample_idx, step_idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_intermediate_samples(images, steps_to_show, n_samples=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E8cfCvajQZcY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
